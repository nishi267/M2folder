Interakt whatsapp API

zipcode regex
^[A-Z]{1,2}[0-9R][0-9A-Z]? [0-9][A-Z]{2}[,. ]?$
postcode_regex = r'\b(?:[A-Za-z][A-Ha-hJ-Yj-y]?[0-9][A-Za-z0-9]? ?[0-9][A-Za-z]{2}|[Gg][Ii][Rr] ?0[Aa]{2})\b'
match = re.search(r"\b[0-9]{3}(?=[,. ]|$)\b", postcode)


phone number

^\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}$

website address:
^(https?://)?(www\.)?[a-zA-Z0-9-]+\.[a-zA-Z]{2,}(\.[a-zA-Z]{2,})?$

email addrress:
^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$


import re

text = "This is a sample text with some VAT and registered information that should be removed. This is some more text that should also be removed."

cleaned_text = re.sub(r'\b(VAT|registered)\b.*', '', text)
cleaned_text = re.sub(r'\s*$', '', cleaned_text)
print(cleaned_text)

original_string = '"sample"'
new_string = original_string.strip('"')
print(new_string)

original_string = '"sample"'
new_string = original_string.replace('"', '')
print(new_string)
###########################################

import nltk
import random

# Download the required resources for the nltk library
nltk.download('wordnet')
nltk.download('punkt')

# Function for synonym replacement
def synonym_replacement(sentence, n):
    # Tokenize the sentence into words
    words = nltk.word_tokenize(sentence)
    
    # Get the WordNet synonym sets for each word
    synsets = [nltk.corpus.wordnet.synsets(word) for word in words]
    
    # Replace n words with their synonyms
    for i in range(n):
        # Choose a random word in the sentence
        idx = random.randint(0, len(words) - 1)
        
        # Choose a random synonym set for the word
        synonyms = synsets[idx]
        if len(synonyms) > 0:
            synonym = random.choice(synonyms).lemmas()[0].name()
            
            # Replace the word with its synonym
            words[idx] = synonym
    
    # Reconstruct the sentence from the modified words
    new_sentence = ' '.join(words)
    return new_sentence

# Function for random insertion
def random_insertion(sentence, n):
    # Tokenize the sentence into words
    words = nltk.word_tokenize(sentence)
    
    # Insert n random words at random positions in the sentence
    for i in range(n):
        word = random.choice(nltk.corpus.words.words())
        idx = random.randint(0, len(words))
        words.insert(idx, word)
    
    # Reconstruct the sentence from the modified words
    new_sentence = ' '.join(words)
    return new_sentence

# Function for random deletion
def random_deletion(sentence, p):
    # Tokenize the sentence into words
    words = nltk.word_tokenize(sentence)
    
    # Delete each word with probability p
    words = [word for word in words if random.uniform(0, 1) > p]
    
    # Reconstruct the sentence from the modified words
    new_sentence = ' '.join(words)
    return new_sentence

# Function for random swap
def random_swap(sentence, n):
    # Tokenize the sentence into words
    words = nltk.word_tokenize(sentence)
    
    # Swap adjacent words n times
    for i in range(n):
        idx = random.randint(0, len(words) - 2)
        words[idx], words[idx+1] = words[idx+1], words[idx]
    
    # Reconstruct the sentence from the modified words
    new_sentence = ' '.join(words)
    return new_sentence



sentence = "The quick brown fox jumps over the lazy dog"
print(synonym_replacement(sentence, 2))
print(random_insertion(sentence, 3))
print(random_deletion(sentence, 0.3))
print(random_swap(sentence, 2))


#GPT-2 method


import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments

# Define the input and output directories
data_dir = "./data"
output_dir = "./output"

# Load the tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Load the minority class data
minority_data = ["some minority class data 1", "some minority class data 2", ...]

# Tokenize the minority class data
minority_tokenized = [tokenizer.encode(text) for text in minority_data]

# Concatenate the tokenized data into a single list
all_tokenized = [tokenizer.encode(text) for text in minority_data]
flat_tokenized = [token for sublist in all_tokenized for token in sublist]

# Create a PyTorch dataset from the tokenized data
dataset = TextDataset(flat_tokenized, tokenizer=tokenizer)

# Define the data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Initialize the GPT-2 model with the appropriate configuration
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Define the training arguments
training_args = TrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    num_train_epochs=10,
    per_device_train_batch_size=4,
    save_steps=1000,
    save_total_limit=5,
    prediction_loss_only=True,
    logging_steps=1000,
    logging_dir='./logs',
    seed=42,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

# Train the model
trainer.train()

# Generate new samples from the model
generated_samples = model.generate(
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
    num_return_sequences=10,
    max_length=50,
)

# Decode the generated samples
generated_texts = [tokenizer.decode(sample, skip_special_tokens=True) for sample in generated_samples]

# Print the generated samples
for text in generated_texts:
    print(text)



##########################################################################
#BERT 


import pandas as pd
import numpy as np
import transformers
from transformers import BertTokenizer, BertForMaskedLM

# Load pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased').cuda()

# Define function to generate synthetic examples
def generate_synthetic_examples(text, num_examples):
    examples = []
    for i in range(num_examples):
        # Mask a random word in the text
        masked_index = np.random.choice(len(text))
        masked_text = list(text)
        masked_text[masked_index] = '[MASK]'
        masked_text = ''.join(masked_text)

        # Convert masked text to input IDs and input mask
        input_ids = tokenizer.encode(masked_text, add_special_tokens=True)
        input_mask = [1] * len(input_ids)

        # Convert input IDs and input mask to tensors
        input_ids = torch.tensor(input_ids).unsqueeze(0).cuda()
        input_mask = torch.tensor(input_mask).unsqueeze(0).cuda()

        # Generate synthetic example using BERT model
        with torch.no_grad():
            output = model(input_ids, attention_mask=input_mask)
            prediction = output[0].argmax(dim=-1)[0, masked_index].item()
            synthetic_text = list(text)
            synthetic_text[masked_index] = tokenizer.convert_ids_to_tokens(prediction)
            synthetic_text = ''.join(synthetic_text)

        examples.append(synthetic_text)

    return examples

# Load text classification dataset
df = pd.read_csv('text_classification_dataset.csv')

# Identify minority class
minority_class = df['label'].value_counts().idxmin()

# Generate synthetic examples for minority class
minority_df = df[df['label'] == minority_class]
num_examples = len(df) - 2*len(minority_df)
synthetic_examples = generate_synthetic_examples(minority_df['text'].iloc[0], num_examples)
synthetic_df = pd.DataFrame({'text': synthetic_examples, 'label': minority_class})

# Concatenate original and synthetic datasets
augmented_df = pd.concat([df, synthetic_df])

# Shuffle dataset
augmented_df = augmented_df.sample(frac=1).reset_index(drop=True)
